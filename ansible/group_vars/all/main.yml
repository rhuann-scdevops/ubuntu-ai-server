---
# =============================================================================
# Ubuntu AI Server - Global Variables
# =============================================================================

# =============================================================================
# Server Information
# =============================================================================
server_hostname: "rhuan-lab-srv02"
server_domain: "home.arpa"
server_fqdn: "{{ server_hostname }}.{{ server_domain }}"
server_ip: "192.168.0.101"
server_timezone: "America/Sao_Paulo"

# =============================================================================
# Storage Configuration
# =============================================================================
# ZFS Fast Pool (SSDs) - For containers, databases, AI models
zfs_fast_pool:
  name: "fast-pool"
  type: "mirror"
  disks:
    - "/dev/sda"
    - "/dev/sdb"
  options:
    compression: "lz4"
    atime: "off"
    xattr: "sa"
    acltype: "posixacl"
  datasets:
    - name: "docker"
      mountpoint: "/fast-pool/docker"
      quota: "100G"
    - name: "databases"
      mountpoint: "/fast-pool/databases"
      quota: "50G"
      recordsize: "8K"  # Optimized for databases
    - name: "ai-models"
      mountpoint: "/fast-pool/ai-models"
      quota: "200G"

# ZFS Bulk Pool (HDDs) - For datasets, backups, media
zfs_bulk_pool:
  name: "bulk-pool"
  type: "mirror"
  disks:
    - "/dev/sdc"
    - "/dev/sdd"
  options:
    compression: "lz4"
    atime: "off"
  datasets:
    - name: "datasets"
      mountpoint: "/bulk-pool/datasets"
      quota: "500G"
    - name: "backups"
      mountpoint: "/bulk-pool/backups"
      quota: "500G"
    - name: "media"
      mountpoint: "/bulk-pool/media"

# =============================================================================
# NVIDIA / CUDA Configuration
# =============================================================================
nvidia:
  driver_version: "550"  # Latest stable driver
  cuda_version: "12.4"
  cudnn_version: "8"
  gpu_model: "Quadro P4000"
  compute_capability: "6.1"  # Pascal architecture

# =============================================================================
# Docker Configuration
# =============================================================================
docker:
  storage_driver: "overlay2"
  data_root: "/fast-pool/docker"
  log_driver: "json-file"
  log_max_size: "100m"
  log_max_file: "3"
  default_network: "172.18.0.0/16"

  # Enable BuildKit
  buildkit: true

  # GPU runtime
  nvidia_runtime: true

# =============================================================================
# AI Stack Configuration
# =============================================================================
ai_stack:
  # Ollama - Local LLM inference
  ollama:
    enabled: true
    port: 11434
    models_path: "/fast-pool/ai-models/ollama"
    default_models:
      - "llama3.2:3b"
      - "codellama:13b"
      - "mistral:7b"
      - "nomic-embed-text"

  # Open WebUI - Chat interface
  open_webui:
    enabled: true
    port: 3000
    data_path: "/fast-pool/docker/open-webui"

  # ComfyUI - Stable Diffusion (disabled - image not available)
  comfyui:
    enabled: false
    port: 8188
    models_path: "/fast-pool/ai-models/comfyui"

  # LocalAI - OpenAI compatible API
  localai:
    enabled: false
    port: 8080
    models_path: "/fast-pool/ai-models/localai"

# =============================================================================
# DevOps Stack Configuration
# =============================================================================
devops_stack:
  # Portainer - Container management
  portainer:
    enabled: true
    port: 9443
    data_path: "/fast-pool/docker/portainer"

  # Traefik - Reverse proxy
  traefik:
    enabled: true
    http_port: 80
    https_port: 443
    dashboard_port: 8080
    config_path: "/fast-pool/docker/traefik"

  # Gitea - Self-hosted Git
  gitea:
    enabled: true
    http_port: 3001
    ssh_port: 2222
    data_path: "/fast-pool/docker/gitea"

  # Drone CI
  drone:
    enabled: false
    port: 8000
    data_path: "/fast-pool/docker/drone"

  # Harbor - Container registry
  harbor:
    enabled: false
    port: 5000
    data_path: "/bulk-pool/docker/harbor"

# =============================================================================
# Monitoring Stack Configuration
# =============================================================================
monitoring:
  # Prometheus
  prometheus:
    enabled: true
    port: 9090
    retention: "30d"
    data_path: "/fast-pool/docker/prometheus"

  # Grafana
  grafana:
    enabled: true
    port: 3002
    admin_password: "ChangeMe!"
    data_path: "/fast-pool/docker/grafana"

  # Loki - Log aggregation
  loki:
    enabled: true
    port: 3100
    data_path: "/bulk-pool/docker/loki"

  # Node Exporter
  node_exporter:
    enabled: true
    port: 9100

  # NVIDIA DCGM Exporter
  dcgm_exporter:
    enabled: true
    port: 9400

# =============================================================================
# Backup Configuration
# =============================================================================
backup:
  # ZFS snapshots
  zfs_snapshots:
    enabled: true
    schedule: "0 2 * * *"  # Daily at 2 AM
    retention_days: 7

  # Remote backup (optional)
  remote:
    enabled: false
    target: ""
    schedule: "0 4 * * 0"  # Weekly on Sunday at 4 AM

# =============================================================================
# Security Configuration
# =============================================================================
security:
  # Firewall (UFW)
  firewall:
    enabled: true
    default_policy: "deny"
    allowed_ports:
      - { port: 22, proto: "tcp", comment: "SSH" }
      - { port: 80, proto: "tcp", comment: "HTTP" }
      - { port: 443, proto: "tcp", comment: "HTTPS" }
      - { port: 3000, proto: "tcp", comment: "Open WebUI" }
      - { port: 3001, proto: "tcp", comment: "Gitea" }
      - { port: 3002, proto: "tcp", comment: "Grafana" }
      - { port: 8080, proto: "tcp", comment: "Traefik Dashboard" }
      - { port: 9090, proto: "tcp", comment: "Prometheus" }
      - { port: 9443, proto: "tcp", comment: "Portainer" }
      - { port: 11434, proto: "tcp", comment: "Ollama" }
      - { port: 8811, proto: "tcp", comment: "MCP Gateway" }
      - { port: 7474, proto: "tcp", comment: "Neo4j Browser" }
      - { port: 7687, proto: "tcp", comment: "Neo4j Bolt" }
      # RAG Stack ports
      - { port: 6333, proto: "tcp", comment: "Qdrant HTTP" }
      - { port: 6334, proto: "tcp", comment: "Qdrant gRPC" }
      - { port: 8085, proto: "tcp", comment: "Docling API" }
      - { port: 8086, proto: "tcp", comment: "Open Interpreter" }
      - { port: 8087, proto: "tcp", comment: "RAG Ingestion" }
      # n8n ports
      - { port: 5678, proto: "tcp", comment: "n8n UI" }
      - { port: 5679, proto: "tcp", comment: "n8n Webhooks" }
      # Docs Generator ports
      - { port: 8088, proto: "tcp", comment: "MkDocs Site" }
      - { port: 8089, proto: "tcp", comment: "Docs API" }
      # LangChain Service
      - { port: 8002, proto: "tcp", comment: "LangChain API" }

  # Fail2ban
  fail2ban:
    enabled: true
    bantime: 3600
    findtime: 600
    maxretry: 5

  # SSH hardening
  ssh:
    permit_root_login: "no"
    password_authentication: "yes"  # Change to "no" after setting up keys
    port: 22

# =============================================================================
# System Packages
# =============================================================================
base_packages:
  - htop
  - iotop
  - iftop
  - ncdu
  - tmux
  - vim
  - curl
  - wget
  - git
  - jq
  - tree
  - unzip
  - net-tools
  - dnsutils
  - nfs-common
  - python3-pip
  - python3-venv

# =============================================================================
# MCP (Model Context Protocol) Servers Configuration
# =============================================================================
# MCP servers enable AI assistants to interact with external systems
# Configure API keys in ansible-vault or secrets file
# =============================================================================
mcp:
  # Paths for MCP data
  config_path: "/fast-pool/docker/mcp/config"
  data_path: "/fast-pool/docker/mcp/data"
  gateway_port: 8811

  # MCP Gateway (disabled - image not available)
  gateway:
    enabled: false

  # ==========================================================================
  # CORE INFRASTRUCTURE MCPs
  # ==========================================================================

  # Desktop Commander - File system access and terminal commands
  desktop_commander:
    enabled: true
    paths:
      - "/home"
      - "/opt"
      - "/fast-pool"
      - "/bulk-pool"
      - "/var/log"

  # GitHub - Repository management, PRs, issues, code search
  github:
    enabled: true
    token: ""  # Set via vault: YOUR_GITHUB_PERSONAL_ACCESS_TOKEN

  # Grafana - Server monitoring integration
  grafana:
    enabled: true
    api_key: ""  # Set via vault: YOUR_GRAFANA_API_KEY

  # Database - PostgreSQL/MySQL/SQLite access
  database:
    enabled: true
    connection_string: "postgresql+asyncpg://mcp:changeme@postgres-mcp:5432/mcp_data"
    user: "mcp"
    password: "changeme"  # Set via vault
    name: "mcp_data"

  # Kubectl - Kubernetes management (if using K8s)
  kubectl:
    enabled: false
    kubeconfig: "/root/.kube/config"

  # ==========================================================================
  # AI & RESEARCH MCPs
  # ==========================================================================

  # Brave Search - Web search for pages, images, news
  brave:
    enabled: true
    api_key: ""  # Set via vault: Get from https://brave.com/search/api/

  # Perplexity - Deep research with web access
  perplexity:
    enabled: true
    api_key: ""  # Set via vault: Get from https://perplexity.ai/

  # Hugging Face - ML models, datasets, papers
  huggingface:
    enabled: true

  # ArXiv - Research paper search and analysis
  arxiv:
    enabled: true
    storage_path: "/bulk-pool/datasets/arxiv"

  # ==========================================================================
  # PRODUCTIVITY & COMMUNICATION MCPs
  # ==========================================================================

  # Notion - Documentation and knowledge management
  notion:
    enabled: false
    token: ""  # Set via vault: YOUR_NOTION_INTEGRATION_TOKEN

  # Slack - Team communication
  slack:
    enabled: false
    bot_token: ""  # Set via vault
    team_id: ""
    channel_ids: ""

  # Gmail - Email integration
  gmail:
    enabled: false
    email_address: ""
    app_password: ""  # Set via vault: Google App Password

  # LinkedIn - Profile scraping and job search
  linkedin:
    enabled: false
    cookie: ""  # Set via vault: li_at cookie from LinkedIn

  # ==========================================================================
  # MEMORY & KNOWLEDGE MCPs
  # ==========================================================================

  # Memory - Basic knowledge graph
  memory:
    enabled: true

  # Neo4j Memory - Advanced graph-based memory
  neo4j:
    enabled: true
    password: "changeme123"  # Set via vault

  # SQLite - Local database with vector search
  sqlite:
    enabled: true

  # ==========================================================================
  # AUTOMATION MCPs
  # ==========================================================================

  # Puppeteer - Browser automation
  puppeteer:
    enabled: true

  # Playwright - Advanced browser automation
  playwright:
    enabled: true
    data_path: "/fast-pool/docker/mcp/playwright"

# =============================================================================
# RAG Stack Configuration (Personal RAG Assistant)
# =============================================================================
# Components for document ingestion, vector storage, and RAG-based retrieval
# =============================================================================
rag_stack:
  # Base paths
  data_path: "/fast-pool/docker/rag"
  documents_path: "/bulk-pool/datasets/documents"

  # Qdrant Vector Database
  qdrant:
    enabled: true
    version: "latest"
    port: 6333
    grpc_port: 6334
    host: "qdrant"

  # Docling Document Parser API
  docling:
    enabled: true
    image: "ds4sd/docling-serve:latest"
    port: 8085

  # Open Interpreter (Code Execution Sandbox)
  interpreter:
    enabled: true
    image: "openinterpreter/open-interpreter:latest"
    port: 8086
    default_model: "codellama:13b"
    gpu_enabled: false  # Set true if you want GPU for code generation

  # AnythingLLM (Alternative RAG UI to OpenWebUI)
  anythingllm:
    enabled: false  # Enable if you prefer AnythingLLM over OpenWebUI RAG
    version: "latest"
    port: 3005
    default_model: "llama3.2:8b"
    auth_token: ""  # Set via vault
    jwt_secret: ""  # Set via vault

  # RAG Ingestion Service (Custom FastAPI)
  ingestion:
    port: 8087

  # Document vendor categories
  vendor_docs:
    - cisco
    - paloalto
    - juniper
    - versa
    - satellite
    - configs

  # Chunking configuration
  chunk_size: 512
  chunk_overlap: 64
  min_chunk_size: 100
  max_chunk_size: 1024
  collection_name: "documents"

  # Retrieval configuration
  default_top_k: 5
  reranking_enabled: false

# =============================================================================
# n8n Workflow Automation Configuration
# =============================================================================
# n8n provides workflow automation for:
#   - GitHub PR automation
#   - Infrastructure monitoring
#   - RAG query orchestration
#   - Multi-service integrations
# =============================================================================
n8n:
  enabled: true
  version: "latest"
  port: 5678
  webhook_port: 5679
  data_path: "/fast-pool/docker/n8n"

  # Database (PostgreSQL recommended for production)
  use_postgres: true
  db_user: "n8n"
  db_password: "changeme"  # Override in vault
  db_name: "n8n"

  # Authentication
  basic_auth_enabled: true
  basic_auth_user: "admin"
  basic_auth_password: "changeme"  # Override in vault

# =============================================================================
# Documentation Generator Configuration
# =============================================================================
# MkDocs Material static site for RAG-generated documentation
# Transforms RAG outputs into publishable Markdown documentation
# =============================================================================
docs_generator:
  enabled: true
  port: 8088
  data_path: "/fast-pool/docker/rag/docs"

  # Site configuration
  site:
    name: "AI-Generated Documentation"
    description: "Technical documentation generated from RAG queries"
    author: "{{ server_hostname }}"
    repo_url: ""  # Optional: Your GitHub docs repo
    repo_name: ""

  # Theme settings (MkDocs Material)
  theme:
    palette:
      primary: "indigo"
      accent: "indigo"
    features:
      - navigation.instant
      - navigation.tracking
      - navigation.tabs
      - navigation.sections
      - navigation.expand
      - search.suggest
      - search.highlight
      - content.code.copy
      - content.tabs.link

  # Document categories
  categories:
    - name: "Runbooks"
      path: "runbooks"
      description: "Operational runbooks and procedures"
    - name: "Guides"
      path: "guides"
      description: "Step-by-step configuration guides"
    - name: "References"
      path: "references"
      description: "Technical reference documentation"
    - name: "Troubleshooting"
      path: "troubleshooting"
      description: "Problem resolution guides"

  # Git integration
  git:
    enabled: true
    auto_commit: true
    branch: "main"
    commit_message_prefix: "[docs-gen]"

# =============================================================================
# LangChain Service Configuration
# =============================================================================
# Unified LangChain API service for RAG pipelines, autonomous agents,
# and conversation management. Integrates with Qdrant and Ollama.
# =============================================================================
langchain_service:
  enabled: true
  version: "latest"
  port: 8002
  data_path: "/fast-pool/docker/langchain"

  # Model configurations - uses models from Ollama
  default_chat_model: "llama3.2:3b"
  default_code_model: "codellama:13b"
  default_embedding_model: "nomic-embed-text"
  noc_expert_model: "llama3.2:3b"  # Use general model as fallback

  # RAG Configuration
  chunk_size: 1000
  chunk_overlap: 200
  retrieval_k: 5

  # Memory Configuration
  memory_type: "buffer"  # buffer, buffer_window, summary, summary_buffer
  memory_k: 10           # For buffer_window
  max_token_limit: 4000  # For summary memory

  # Agent Configuration
  agent_max_iterations: 15
  agent_timeout: 120
