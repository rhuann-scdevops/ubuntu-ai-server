"""
Conversation Memory Management
Memory implementations for maintaining conversation context.
Adapted for LangChain 1.x using core message types.
"""
from typing import Optional, Dict, Any, List
from datetime import datetime
import json
from pathlib import Path
from collections import deque

from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from config import get_settings


class MemoryManager:
    """Memory manager for conversation context using LangChain 1.x patterns."""

    def __init__(
        self,
        memory_type: str = "buffer",
        memory_key: str = "chat_history",
        **kwargs
    ):
        self.settings = get_settings()
        self.memory_type = memory_type
        self.memory_key = memory_key
        self.kwargs = kwargs

        # Store messages directly
        self._messages: List[BaseMessage] = []

        # For window memory
        self._k = kwargs.get("k", self.settings.memory_k)

        # For summary memory
        self._summary = ""
        self._max_token_limit = kwargs.get("max_token_limit", self.settings.max_token_limit)

        # LLM for summarization
        if memory_type in ["summary", "summary_buffer"]:
            self._llm = ChatOllama(
                base_url=self.settings.ollama_url,
                model=self.settings.default_chat_model,
                temperature=0
            )

    def add_message(self, human_input: str, ai_output: str):
        """Add a conversation exchange to memory."""
        self._messages.append(HumanMessage(content=human_input))
        self._messages.append(AIMessage(content=ai_output))

        # Handle memory type specific logic
        if self.memory_type == "buffer_window":
            # Keep only last k exchanges (k * 2 messages)
            max_messages = self._k * 2
            if len(self._messages) > max_messages:
                self._messages = self._messages[-max_messages:]

        elif self.memory_type == "summary":
            # Summarize all messages
            self._update_summary()

        elif self.memory_type == "summary_buffer":
            # Summarize when exceeding token limit (approximate)
            total_chars = sum(len(m.content) for m in self._messages)
            # Rough estimate: 4 chars per token
            if total_chars / 4 > self._max_token_limit:
                self._update_summary()
                # Keep only recent messages after summarization
                self._messages = self._messages[-4:]

    def _update_summary(self):
        """Update the conversation summary."""
        if not self._messages:
            return

        history_text = self.get_buffer_string()

        summary_prompt = ChatPromptTemplate.from_template(
            """Progressively summarize the conversation below, keeping key information:

Current summary: {current_summary}

New conversation:
{history}

Updated summary:"""
        )

        chain = summary_prompt | self._llm | StrOutputParser()
        self._summary = chain.invoke({
            "current_summary": self._summary or "No previous summary.",
            "history": history_text
        })

    def get_history(self) -> List[BaseMessage]:
        """Get conversation history as messages."""
        if self.memory_type == "summary" and self._summary:
            return [SystemMessage(content=f"Conversation summary: {self._summary}")]
        elif self.memory_type == "summary_buffer" and self._summary:
            return [SystemMessage(content=f"Previous summary: {self._summary}")] + self._messages
        return self._messages.copy()

    def get_history_as_tuples(self) -> List[tuple]:
        """Get conversation history as list of (human, ai) tuples."""
        history = []
        for i in range(0, len(self._messages) - 1, 2):
            if i + 1 < len(self._messages):
                human = self._messages[i].content if isinstance(self._messages[i], HumanMessage) else ""
                ai = self._messages[i + 1].content if isinstance(self._messages[i + 1], AIMessage) else ""
                history.append((human, ai))
        return history

    def clear(self):
        """Clear conversation history."""
        self._messages = []
        self._summary = ""

    def get_buffer_string(self) -> str:
        """Get formatted conversation history as string."""
        lines = []
        for msg in self._messages:
            role = "Human" if isinstance(msg, HumanMessage) else "AI"
            lines.append(f"{role}: {msg.content}")
        return "\n".join(lines)

    def get_context_dict(self) -> Dict[str, Any]:
        """Get memory variables as dict for use in chains."""
        return {self.memory_key: self.get_history()}


class ConversationStore:
    """Persistent conversation storage."""

    def __init__(self, storage_path: str = "/tmp/langchain_conversations"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)

    def save_conversation(
        self,
        session_id: str,
        messages: List[Dict[str, str]],
        metadata: Optional[Dict[str, Any]] = None
    ):
        """Save a conversation to storage."""
        conversation = {
            "session_id": session_id,
            "timestamp": datetime.utcnow().isoformat(),
            "messages": messages,
            "metadata": metadata or {}
        }

        file_path = self.storage_path / f"{session_id}.json"
        with open(file_path, "w") as f:
            json.dump(conversation, f, indent=2)

    def load_conversation(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Load a conversation from storage."""
        file_path = self.storage_path / f"{session_id}.json"
        if file_path.exists():
            with open(file_path, "r") as f:
                return json.load(f)
        return None

    def list_conversations(self) -> List[Dict[str, Any]]:
        """List all stored conversations."""
        conversations = []
        for file_path in self.storage_path.glob("*.json"):
            try:
                with open(file_path, "r") as f:
                    data = json.load(f)
                    conversations.append({
                        "session_id": data["session_id"],
                        "timestamp": data["timestamp"],
                        "message_count": len(data["messages"]),
                        "metadata": data.get("metadata", {})
                    })
            except Exception:
                continue
        return sorted(conversations, key=lambda x: x["timestamp"], reverse=True)

    def delete_conversation(self, session_id: str) -> bool:
        """Delete a conversation from storage."""
        file_path = self.storage_path / f"{session_id}.json"
        if file_path.exists():
            file_path.unlink()
            return True
        return False

    def add_message_to_conversation(
        self,
        session_id: str,
        role: str,
        content: str
    ):
        """Add a message to an existing conversation."""
        conversation = self.load_conversation(session_id)
        if conversation is None:
            conversation = {
                "session_id": session_id,
                "timestamp": datetime.utcnow().isoformat(),
                "messages": [],
                "metadata": {}
            }

        conversation["messages"].append({
            "role": role,
            "content": content,
            "timestamp": datetime.utcnow().isoformat()
        })

        self.save_conversation(
            session_id,
            conversation["messages"],
            conversation.get("metadata")
        )


def create_memory(
    memory_type: str = None,
    **kwargs
) -> MemoryManager:
    """Factory function to create a memory manager."""
    settings = get_settings()
    memory_type = memory_type or settings.memory_type

    return MemoryManager(
        memory_type=memory_type,
        **kwargs
    )
