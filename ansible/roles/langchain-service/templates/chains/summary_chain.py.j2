"""
Summary Chains
Chains for summarizing documents, logs, and other content.
Adapted for LangChain 1.x using LCEL.
"""
from typing import Optional, List, Dict, Any
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_ollama import ChatOllama
from langchain_text_splitters import RecursiveCharacterTextSplitter

from config import get_settings, MODEL_CONFIGS


class SummaryChain:
    """Chain for summarizing various types of content using LCEL."""

    # Summary prompts for different content types
    PROMPTS = {
        "general": """Write a concise summary of the following content:

{text}

CONCISE SUMMARY:""",

        "technical": """Provide a technical summary of the following documentation:

{text}

Include:
- Key concepts and features
- Important configurations
- Notable limitations or requirements

TECHNICAL SUMMARY:""",

        "log": """Analyze and summarize the following log entries:

{text}

Include:
- Overall status/health assessment
- Key events or errors found
- Patterns or anomalies detected
- Recommended actions if any

LOG ANALYSIS SUMMARY:""",

        "config": """Summarize the following configuration:

{text}

Include:
- Main purpose/function
- Key settings and their values
- Security considerations
- Dependencies or requirements

CONFIGURATION SUMMARY:""",

        "incident": """Summarize the following incident report or troubleshooting session:

{text}

Include:
- Problem description
- Root cause (if identified)
- Resolution steps taken
- Prevention recommendations

INCIDENT SUMMARY:"""
    }

    def __init__(
        self,
        model_name: Optional[str] = None,
        temperature: float = 0.3,
        chain_type: str = "stuff"
    ):
        self.settings = get_settings()
        self.model_name = model_name or self.settings.default_chat_model
        self.temperature = temperature
        self.chain_type = chain_type

        self.llm = ChatOllama(
            base_url=self.settings.ollama_url,
            model=self.model_name,
            temperature=self.temperature
        )

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.settings.chunk_size,
            chunk_overlap=self.settings.chunk_overlap
        )

    def _build_chain(self, prompt_type: str = "general"):
        """Build LCEL summary chain for the specified prompt type."""
        prompt_template = self.PROMPTS.get(prompt_type, self.PROMPTS["general"])
        prompt = ChatPromptTemplate.from_template(prompt_template)
        return prompt | self.llm | StrOutputParser()

    def _combine_docs(self, docs: List[Document]) -> str:
        """Combine multiple documents into a single text string."""
        return "\n\n".join(doc.page_content for doc in docs)

    async def asummarize(
        self,
        text: str,
        prompt_type: str = "general"
    ) -> str:
        """Async summarize text content."""
        docs = [Document(page_content=text)]

        if len(text) > self.settings.chunk_size * 2:
            docs = self.text_splitter.create_documents([text])

        # For stuff chain type, combine all docs and summarize
        combined_text = self._combine_docs(docs)
        chain = self._build_chain(prompt_type)
        return await chain.ainvoke({"text": combined_text})

    def summarize(
        self,
        text: str,
        prompt_type: str = "general"
    ) -> str:
        """Sync summarize text content."""
        docs = [Document(page_content=text)]

        if len(text) > self.settings.chunk_size * 2:
            docs = self.text_splitter.create_documents([text])

        # For stuff chain type, combine all docs and summarize
        combined_text = self._combine_docs(docs)
        chain = self._build_chain(prompt_type)
        return chain.invoke({"text": combined_text})


class LogSummaryChain(SummaryChain):
    """Specialized chain for log analysis and summarization."""

    def __init__(self, model_name: Optional[str] = None):
        super().__init__(
            model_name=model_name,
            temperature=0.2,
            chain_type="stuff"
        )

    async def analyze_logs(self, log_content: str) -> Dict[str, Any]:
        """Analyze logs and provide structured output."""
        analysis_prompt = ChatPromptTemplate.from_template("""Analyze the following logs and provide a structured analysis:

{logs}

Provide your analysis in the following format:

## Status
[Overall health: HEALTHY/WARNING/CRITICAL]

## Key Events
- [List important events]

## Errors Found
- [List any errors with timestamps if available]

## Patterns
- [Any recurring patterns or anomalies]

## Recommendations
- [Suggested actions]

ANALYSIS:""")

        chain = analysis_prompt | self.llm | StrOutputParser()
        analysis = await chain.ainvoke({"logs": log_content})

        return {
            "analysis": analysis,
            "log_length": len(log_content),
            "line_count": log_content.count('\n') + 1
        }

    def analyze_logs_sync(self, log_content: str) -> Dict[str, Any]:
        """Sync analyze logs and provide structured output."""
        analysis_prompt = ChatPromptTemplate.from_template("""Analyze the following logs and provide a structured analysis:

{logs}

Provide your analysis in the following format:

## Status
[Overall health: HEALTHY/WARNING/CRITICAL]

## Key Events
- [List important events]

## Errors Found
- [List any errors with timestamps if available]

## Patterns
- [Any recurring patterns or anomalies]

## Recommendations
- [Suggested actions]

ANALYSIS:""")

        chain = analysis_prompt | self.llm | StrOutputParser()
        analysis = chain.invoke({"logs": log_content})

        return {
            "analysis": analysis,
            "log_length": len(log_content),
            "line_count": log_content.count('\n') + 1
        }


class ConfigSummaryChain(SummaryChain):
    """Specialized chain for configuration analysis."""

    def __init__(self, model_name: Optional[str] = None):
        super().__init__(
            model_name=model_name,
            temperature=0.2,
            chain_type="stuff"
        )

    async def analyze_config(
        self,
        config_content: str,
        config_type: str = "generic"
    ) -> Dict[str, Any]:
        """Analyze configuration and provide insights."""
        analysis_prompt = ChatPromptTemplate.from_template("""Analyze the following {config_type} configuration:

{config}

Provide your analysis including:

## Purpose
[What this configuration does]

## Key Settings
| Setting | Value | Description |
|---------|-------|-------------|
[List important settings]

## Security Assessment
- [Any security concerns or best practices]

## Dependencies
- [Required services, ports, or configurations]

## Recommendations
- [Optimization or improvement suggestions]

ANALYSIS:""")

        chain = analysis_prompt | self.llm | StrOutputParser()
        analysis = await chain.ainvoke({
            "config": config_content,
            "config_type": config_type
        })

        return {
            "analysis": analysis,
            "config_type": config_type,
            "config_length": len(config_content)
        }

    def analyze_config_sync(
        self,
        config_content: str,
        config_type: str = "generic"
    ) -> Dict[str, Any]:
        """Sync analyze configuration and provide insights."""
        analysis_prompt = ChatPromptTemplate.from_template("""Analyze the following {config_type} configuration:

{config}

Provide your analysis including:

## Purpose
[What this configuration does]

## Key Settings
| Setting | Value | Description |
|---------|-------|-------------|
[List important settings]

## Security Assessment
- [Any security concerns or best practices]

## Dependencies
- [Required services, ports, or configurations]

## Recommendations
- [Optimization or improvement suggestions]

ANALYSIS:""")

        chain = analysis_prompt | self.llm | StrOutputParser()
        analysis = chain.invoke({
            "config": config_content,
            "config_type": config_type
        })

        return {
            "analysis": analysis,
            "config_type": config_type,
            "config_length": len(config_content)
        }


def create_summary_chain(
    summary_type: str = "general",
    model_config: str = "general"
) -> SummaryChain:
    """Factory function to create summary chains."""
    config = MODEL_CONFIGS.get(model_config, MODEL_CONFIGS["general"])

    if summary_type == "log":
        return LogSummaryChain(model_name=config["model"])
    elif summary_type == "config":
        return ConfigSummaryChain(model_name=config["model"])

    return SummaryChain(
        model_name=config["model"],
        temperature=config["temperature"]
    )
