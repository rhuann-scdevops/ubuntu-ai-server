"""
RAG (Retrieval Augmented Generation) Chains
Chains for document-based Q&A with context retrieval.
Adapted for Qdrant vector store using LCEL.
"""
from typing import Optional, List, Dict, Any
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from langchain_ollama import ChatOllama
from langchain_qdrant import QdrantVectorStore

from config import get_settings, MODEL_CONFIGS


def format_docs(docs: List[Document]) -> str:
    """Format documents into a single string for context."""
    return "\n\n".join(doc.page_content for doc in docs)


class RAGChain:
    """RAG Chain for document-based question answering with Qdrant using LCEL."""

    def __init__(
        self,
        vectorstore: QdrantVectorStore,
        model_name: Optional[str] = None,
        temperature: float = 0.3,
        k: int = 5
    ):
        self.settings = get_settings()
        self.vectorstore = vectorstore
        self.model_name = model_name or self.settings.default_chat_model
        self.temperature = temperature
        self.k = k

        self.llm = ChatOllama(
            base_url=self.settings.ollama_url,
            model=self.model_name,
            temperature=self.temperature
        )

        self.retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": self.k}
        )

        self.chain = self._build_chain()

    def _build_chain(self):
        """Build the RAG chain using LCEL."""
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful AI assistant with access to a knowledge base.
Use the following context to answer the user's question accurately.
If the context doesn't contain relevant information, say so clearly.
Always cite your sources when possible.

Context:
{context}"""),
            ("human", "{question}")
        ])

        # LCEL chain: retrieve -> format -> prompt -> llm -> parse
        chain = (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return chain

    async def ainvoke(self, question: str) -> Dict[str, Any]:
        """Async invoke the RAG chain."""
        # Get documents for sources
        docs = await self.retriever.ainvoke(question)

        # Run the chain
        answer = await self.chain.ainvoke(question)

        return {
            "answer": answer,
            "sources": [
                {
                    "content": doc.page_content[:500],
                    "metadata": doc.metadata
                }
                for doc in docs
            ]
        }

    def invoke(self, question: str) -> Dict[str, Any]:
        """Sync invoke the RAG chain."""
        # Get documents for sources
        docs = self.retriever.invoke(question)

        # Run the chain
        answer = self.chain.invoke(question)

        return {
            "answer": answer,
            "sources": [
                {
                    "content": doc.page_content[:500],
                    "metadata": doc.metadata
                }
                for doc in docs
            ]
        }


def create_rag_chain(
    vectorstore: QdrantVectorStore,
    model_config: str = "general",
    k: int = 5
) -> RAGChain:
    """Factory function to create a RAG chain with predefined model config."""
    config = MODEL_CONFIGS.get(model_config, MODEL_CONFIGS["general"])
    return RAGChain(
        vectorstore=vectorstore,
        model_name=config["model"],
        temperature=config["temperature"],
        k=k
    )


class ConversationalRAGChain:
    """Conversational RAG chain with chat history support using LCEL."""

    def __init__(
        self,
        vectorstore: QdrantVectorStore,
        model_name: Optional[str] = None,
        temperature: float = 0.3,
        k: int = 5
    ):
        self.settings = get_settings()
        self.vectorstore = vectorstore
        self.model_name = model_name or self.settings.default_chat_model
        self.temperature = temperature
        self.k = k

        self.llm = ChatOllama(
            base_url=self.settings.ollama_url,
            model=self.model_name,
            temperature=self.temperature
        )

        self.retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": self.k}
        )

    def _contextualize_question(self, question: str, chat_history: List[tuple]) -> str:
        """Reformulate question based on chat history."""
        if not chat_history:
            return question

        # Build history context
        history_str = "\n".join([f"Human: {h}\nAI: {a}" for h, a in chat_history[-3:]])

        contextualize_prompt = ChatPromptTemplate.from_messages([
            ("system", """Given a chat history and the latest user question,
formulate a standalone question that can be understood without the chat history.
Do NOT answer the question, just reformulate it if needed.
If the question is already standalone, return it as-is.

Chat history:
{history}"""),
            ("human", "{question}")
        ])

        chain = contextualize_prompt | self.llm | StrOutputParser()
        return chain.invoke({"history": history_str, "question": question})

    async def ainvoke(
        self,
        question: str,
        chat_history: List[tuple] = None
    ) -> Dict[str, Any]:
        """Async invoke with chat history."""
        # Contextualize the question if there's history
        contextualized_q = question
        if chat_history:
            contextualized_q = self._contextualize_question(question, chat_history)

        # Get documents
        docs = await self.retriever.ainvoke(contextualized_q)
        context = format_docs(docs)

        # Build history string
        history_str = ""
        if chat_history:
            history_str = "\n".join([f"Human: {h}\nAI: {a}" for h, a in chat_history[-3:]])

        # Answer prompt
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful AI assistant with access to a knowledge base.
Use the following context to answer the user's question accurately.
If the context doesn't contain relevant information, say so clearly.

Context:
{context}

Previous conversation:
{history}"""),
            ("human", "{question}")
        ])

        chain = qa_prompt | self.llm | StrOutputParser()
        answer = await chain.ainvoke({
            "context": context,
            "history": history_str,
            "question": question
        })

        return {
            "answer": answer,
            "sources": [
                {
                    "content": doc.page_content[:500],
                    "metadata": doc.metadata
                }
                for doc in docs
            ]
        }

    def invoke(
        self,
        question: str,
        chat_history: List[tuple] = None
    ) -> Dict[str, Any]:
        """Sync invoke with chat history."""
        # Contextualize the question if there's history
        contextualized_q = question
        if chat_history:
            contextualized_q = self._contextualize_question(question, chat_history)

        # Get documents
        docs = self.retriever.invoke(contextualized_q)
        context = format_docs(docs)

        # Build history string
        history_str = ""
        if chat_history:
            history_str = "\n".join([f"Human: {h}\nAI: {a}" for h, a in chat_history[-3:]])

        # Answer prompt
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful AI assistant with access to a knowledge base.
Use the following context to answer the user's question accurately.
If the context doesn't contain relevant information, say so clearly.

Context:
{context}

Previous conversation:
{history}"""),
            ("human", "{question}")
        ])

        chain = qa_prompt | self.llm | StrOutputParser()
        answer = chain.invoke({
            "context": context,
            "history": history_str,
            "question": question
        })

        return {
            "answer": answer,
            "sources": [
                {
                    "content": doc.page_content[:500],
                    "metadata": doc.metadata
                }
                for doc in docs
            ]
        }


def create_conversational_rag_chain(
    vectorstore: QdrantVectorStore,
    model_config: str = "general",
    k: int = 5
) -> ConversationalRAGChain:
    """Factory function to create a conversational RAG chain."""
    config = MODEL_CONFIGS.get(model_config, MODEL_CONFIGS["general"])
    return ConversationalRAGChain(
        vectorstore=vectorstore,
        model_name=config["model"],
        temperature=config["temperature"],
        k=k
    )
